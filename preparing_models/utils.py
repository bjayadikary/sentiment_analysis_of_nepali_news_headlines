# Tokenizer
def get_words(sentence):
    return [word for word in sentence.split()]