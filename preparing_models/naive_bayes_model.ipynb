{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662d5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import my_stemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0ace3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42dde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0fc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_dataset(training_data_file_path):\n",
    "    headlines = []\n",
    "    labels = []\n",
    "    with open(training_data_file_path, 'r', encoding='utf8') as tsv_file:\n",
    "        filereader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        \n",
    "        for row in filereader:\n",
    "            headlines.append(row[0])\n",
    "            labels.append(int(row[4]))\n",
    "            \n",
    "    return headlines, labels\n",
    "\n",
    "\n",
    "def create_dictionary(headlines):\n",
    "    temp_counter = {}\n",
    "    vocabs = {}\n",
    "    i = 0\n",
    "    for headline in headlines:\n",
    "        words = utils.get_words(headline)\n",
    "        for word in words:\n",
    "            if word not in temp_counter:\n",
    "                temp_counter[word] = 1\n",
    "            else:\n",
    "                temp_counter[word] += 1\n",
    "                \n",
    "            # only considering word into dictionary if it's occurence exceeds given threshold\n",
    "            if temp_counter[word] == 1:\n",
    "                vocabs[word] = i\n",
    "                i += 1\n",
    "    return vocabs\n",
    "\n",
    "\n",
    "def bag_of_words(headlines, word_dictionary, multinomial):\n",
    "    N, V = len(headlines), len(word_dictionary)\n",
    "    data = np.zeros((N, V))\n",
    "    for i, headline in enumerate(headlines):\n",
    "        for word in utils.get_words(headline):\n",
    "            if word in word_dictionary:\n",
    "                if multinomial:\n",
    "                    data[i, word_dictionary[word]] += 1 # Multinomial Naive Bayes\n",
    "                else:\n",
    "                    data[i, word_dictionary[word]] = 1 # Bernoulli event model\n",
    "    return data \n",
    "\n",
    "\n",
    "def refine_to_one_vs_rest_dataset(headlines, labels, one_label=None):\n",
    "    one_vs_rest_labels = []\n",
    "    if one_label is not None:\n",
    "        for label in labels:\n",
    "            one_vs_rest_labels.append(1 if label == one_label else 0)\n",
    "    else:\n",
    "        raise TypeError(\"Missing positional argument: one_label\")\n",
    "        \n",
    "    return headlines, np.array(one_vs_rest_labels)\n",
    "\n",
    "\n",
    "def fit_naive_bayes_model(matrix, labels, multinomial):\n",
    "    total_headlines_in_label_one = np.sum(labels)\n",
    "    phi_y = labels.sum(axis=0) / np.shape(labels)\n",
    "    \n",
    "    phi_x = np.zeros((2, matrix.shape[1]))\n",
    "    phi_x[0] = matrix[labels == 0].sum(axis=0)\n",
    "    phi_x[1] = matrix[labels == 1].sum(axis=0)\n",
    "    \n",
    "    # Laplace smoothing (for multinomial event)\n",
    "    if multinomial:\n",
    "        phi_x += 1\n",
    "        phi_x = phi_x / phi_x.sum(axis=1, keepdims=True)\n",
    "    else:\n",
    "        # Laplace smoothing (for bernoulli event)\n",
    "        phi_x += 1\n",
    "        phi_x[0] = phi_x[0] / (len(labels) - total_headlines_in_label_one + 3)\n",
    "        phi_x[1] = phi_x[1] / (total_headlines_in_label_one + 3)\n",
    "    \n",
    "    return phi_x, phi_y\n",
    "\n",
    "\n",
    "def predict_from_naive_bayes_model(model, matrix):\n",
    "    if matrix.ndim == 1:\n",
    "        matrix = np.expand_dims(matrix, axis=-2)\n",
    "    phi_x, phi_y = model\n",
    "    log_likelihood = np.sum(matrix[:, None] * np.log(phi_x[None]), axis=-1)\n",
    "    log_likelihood[:, 0] += np.log(1 - phi_y)\n",
    "    log_likelihood[:, 1] += np.log(phi_y)\n",
    "    \n",
    "    return np.array(log_likelihood)\n",
    "\n",
    "def stem_headlines(headlines):\n",
    "    stemmed_headlines = []\n",
    "    for i, headline in enumerate(headlines):\n",
    "        result_from_my_stemmer = my_stemmer.stem_it(headline)\n",
    "        if i%10 == 0:\n",
    "            print(f\"{i} Headlines Stemming Complete\")\n",
    "        stemmed_headlines.append([result_from_my_stemmer])\n",
    "        \n",
    "    return stemmed_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc31af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53d9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_model(train_headlines, test_headlines, raw_train_labels, raw_test_labels, multinomial=False):\n",
    "    # maintaining list to store likelihood for each category\n",
    "    log_likelihoods = []\n",
    "    \n",
    "    # Creating dictionary of words\n",
    "    dictionary = create_dictionary(train_headlines)\n",
    "    print(f\"Total dictionary words: {len(dictionary)}\")\n",
    "    \n",
    "    train_matrix = bag_of_words(train_headlines, dictionary, multinomial)\n",
    "    test_matrix = bag_of_words(test_headlines, dictionary, multinomial)\n",
    "      \n",
    "    classification_labels = [0, 1, 2]\n",
    "    \n",
    "    for class_label in classification_labels:\n",
    "        \n",
    "        # refine dataset to fit for one-vs-rest\n",
    "        train_headlines, train_labels = refine_to_one_vs_rest_dataset(train_headlines, raw_train_labels, class_label)\n",
    "        test_headlines, test_labels = refine_to_one_vs_rest_dataset(test_headlines, raw_test_labels, class_label)\n",
    "        \n",
    "        # Fit and predict\n",
    "        model = fit_naive_bayes_model(train_matrix, train_labels, multinomial)\n",
    "        log_likelihood = predict_from_naive_bayes_model(model, test_matrix)\n",
    "        log_likelihoods.append(log_likelihood[:, 1])\n",
    "        \n",
    "    # Calculating accuracy\n",
    "    # TO check why reshaping and changing axis=1 does give only about 1/3 accuracy\n",
    "#     log_likelihoods = np.reshape(log_likelihoods, (np.shape(log_likelihoods)[-1], np.shape(log_likelihoods)[-2]))\n",
    "    test_predictions = np.argmax(log_likelihoods, axis=0)\n",
    "    accuracy = np.mean([test_predictions == raw_test_labels])\n",
    "    print(f\"Accuracy obtained: {accuracy}\")\n",
    "    \n",
    "    if multinomial:\n",
    "        # Fit and predict using MultinomialNB Sklearn\n",
    "        MultiNB = MultinomialNB()\n",
    "        MultiNB.fit(train_matrix, raw_train_labels)\n",
    "        sklearn_multinomial_predictions = MultiNB.predict(test_matrix)\n",
    "        sklearn_multinomial_accuracy = np.mean([sklearn_multinomial_predictions == raw_test_labels])\n",
    "        print(f\"Multinomial sklearn: {sklearn_multinomial_accuracy}\")\n",
    "    \n",
    "    else:\n",
    "        # Fit and predict using Sklearn(Bernoulli)   \n",
    "        BernNB = BernoulliNB()\n",
    "        BernNB.fit(train_matrix, raw_train_labels)\n",
    "        sklearn_bernoulli_predictions = BernNB.predict(test_matrix)\n",
    "        sklearn_bernoulli_accuracy = np.mean([sklearn_bernoulli_predictions == raw_test_labels])\n",
    "        print(f\"Bernoulli sklearn: {sklearn_bernoulli_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0b0f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Headlines Stemming Complete\n",
      "10 Headlines Stemming Complete\n",
      "20 Headlines Stemming Complete\n",
      "30 Headlines Stemming Complete\n",
      "40 Headlines Stemming Complete\n",
      "50 Headlines Stemming Complete\n",
      "60 Headlines Stemming Complete\n",
      "70 Headlines Stemming Complete\n",
      "80 Headlines Stemming Complete\n",
      "90 Headlines Stemming Complete\n",
      "100 Headlines Stemming Complete\n",
      "110 Headlines Stemming Complete\n",
      "120 Headlines Stemming Complete\n",
      "130 Headlines Stemming Complete\n",
      "140 Headlines Stemming Complete\n",
      "150 Headlines Stemming Complete\n",
      "160 Headlines Stemming Complete\n",
      "170 Headlines Stemming Complete\n",
      "180 Headlines Stemming Complete\n",
      "190 Headlines Stemming Complete\n",
      "200 Headlines Stemming Complete\n",
      "210 Headlines Stemming Complete\n",
      "220 Headlines Stemming Complete\n",
      "230 Headlines Stemming Complete\n",
      "240 Headlines Stemming Complete\n",
      "250 Headlines Stemming Complete\n",
      "260 Headlines Stemming Complete\n",
      "270 Headlines Stemming Complete\n",
      "280 Headlines Stemming Complete\n",
      "290 Headlines Stemming Complete\n",
      "300 Headlines Stemming Complete\n",
      "310 Headlines Stemming Complete\n",
      "320 Headlines Stemming Complete\n",
      "330 Headlines Stemming Complete\n",
      "340 Headlines Stemming Complete\n",
      "350 Headlines Stemming Complete\n",
      "360 Headlines Stemming Complete\n",
      "370 Headlines Stemming Complete\n",
      "380 Headlines Stemming Complete\n",
      "390 Headlines Stemming Complete\n",
      "400 Headlines Stemming Complete\n",
      "410 Headlines Stemming Complete\n",
      "420 Headlines Stemming Complete\n",
      "430 Headlines Stemming Complete\n",
      "440 Headlines Stemming Complete\n",
      "450 Headlines Stemming Complete\n",
      "460 Headlines Stemming Complete\n",
      "470 Headlines Stemming Complete\n",
      "480 Headlines Stemming Complete\n",
      "490 Headlines Stemming Complete\n",
      "500 Headlines Stemming Complete\n",
      "510 Headlines Stemming Complete\n",
      "520 Headlines Stemming Complete\n",
      "530 Headlines Stemming Complete\n",
      "540 Headlines Stemming Complete\n",
      "550 Headlines Stemming Complete\n",
      "560 Headlines Stemming Complete\n",
      "570 Headlines Stemming Complete\n",
      "580 Headlines Stemming Complete\n",
      "590 Headlines Stemming Complete\n",
      "600 Headlines Stemming Complete\n",
      "610 Headlines Stemming Complete\n",
      "620 Headlines Stemming Complete\n",
      "630 Headlines Stemming Complete\n",
      "640 Headlines Stemming Complete\n",
      "650 Headlines Stemming Complete\n",
      "660 Headlines Stemming Complete\n",
      "670 Headlines Stemming Complete\n",
      "680 Headlines Stemming Complete\n",
      "690 Headlines Stemming Complete\n",
      "0 Headlines Stemming Complete\n",
      "10 Headlines Stemming Complete\n",
      "20 Headlines Stemming Complete\n",
      "30 Headlines Stemming Complete\n",
      "40 Headlines Stemming Complete\n",
      "50 Headlines Stemming Complete\n",
      "60 Headlines Stemming Complete\n",
      "70 Headlines Stemming Complete\n",
      "80 Headlines Stemming Complete\n",
      "90 Headlines Stemming Complete\n",
      "100 Headlines Stemming Complete\n",
      "110 Headlines Stemming Complete\n",
      "120 Headlines Stemming Complete\n",
      "130 Headlines Stemming Complete\n",
      "140 Headlines Stemming Complete\n",
      "150 Headlines Stemming Complete\n",
      "160 Headlines Stemming Complete\n",
      "170 Headlines Stemming Complete\n",
      "180 Headlines Stemming Complete\n",
      "190 Headlines Stemming Complete\n",
      "200 Headlines Stemming Complete\n",
      "210 Headlines Stemming Complete\n",
      "220 Headlines Stemming Complete\n",
      "230 Headlines Stemming Complete\n",
      "240 Headlines Stemming Complete\n",
      "250 Headlines Stemming Complete\n",
      "260 Headlines Stemming Complete\n",
      "270 Headlines Stemming Complete\n",
      "280 Headlines Stemming Complete\n",
      "290 Headlines Stemming Complete\n"
     ]
    }
   ],
   "source": [
    "training_data_file_path = r'/home/jay/projectWorks/Inception/data_preprocessing/data_to_label/final_data_to_label/final_training_data/first_training_data.tsv'\n",
    "\n",
    "# Loading headlines and raw_labels\n",
    "headlines, raw_labels = load_news_dataset(training_data_file_path)\n",
    "\n",
    "# Splitting dataset for training and validation set\n",
    "train_headlines, test_headlines, raw_train_labels, raw_test_labels = train_test_split(headlines, raw_labels, test_size=0.3, random_state=1)\n",
    "\n",
    "# Stemming\n",
    "stemmed_train_headlines = stem_headlines(train_headlines)\n",
    "stemmed_test_headlines = stem_headlines(test_headlines)\n",
    "\n",
    "stemmed_train_headlines = np.reshape(stemmed_train_headlines, (len(stemmed_train_headlines), ))\n",
    "stemmed_test_headlines = np.reshape(stemmed_test_headlines, (len(stemmed_test_headlines), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9322f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary words: 3618\n",
      "Accuracy obtained: 0.6287625418060201\n",
      "Multinomial sklearn: 0.6287625418060201\n"
     ]
    }
   ],
   "source": [
    "# Fitting multinomial NB\n",
    "naive_bayes_model(train_headlines, test_headlines, raw_train_labels, raw_test_labels, multinomial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2897c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fbc3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dictionary words: 3618\n",
      "Accuracy obtained: 0.5819397993311036\n",
      "Bernoulli sklearn: 0.568561872909699\n"
     ]
    }
   ],
   "source": [
    "# Fitting bernoulli NB\n",
    "naive_bayes_model(train_headlines, test_headlines, raw_train_labels, raw_test_labels, multinomial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f0f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
